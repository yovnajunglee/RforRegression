---
title: "R for Regression: Intro to Regression in R"
subtitle: "STA2005S, 2021"
author: "Yovna Junglee"
output: learnr::tutorial
runtime: shiny_prerendered
description: >
  Learn how to implement linear regression in R from first principles and using the `lm()` function.
---

```{r setup, include=FALSE}
library(learnr)
library(corrplot)
knitr::opts_chunk$set(echo = FALSE, eval=TRUE)
adverts <- read.csv("Advertising.csv", header=TRUE)[,-1]
#attach(adverts)
nobs <- nrow(adverts)
### Simple linear regression
X <- cbind(1,adverts$TV)
Y <- adverts$sales
bhat <- solve(t(X)%*%X)%*%t(X)%*%Y
C <- solve(t(X)%*%X)
# Find k
k <- 1+1
# Find s2
rss <- t(Y-X%*%bhat)%*%(Y-X%*%bhat)
s2 <- as.numeric(rss)/(nobs-k)
# Find the diagonals of the C matrix
c_ii <- diag(C)
# Calculate the standard error
std.error <- sqrt(s2*c_ii)
### Multiple linear regression
Xb <- as.matrix(cbind(1,adverts[,1:3]))
bhatb<- solve(t(Xb)%*%Xb)%*%t(Xb)%*%Y
# Find k
k <- 1+3
# Find s2
rssb <- t(Y-Xb%*%bhatb)%*%(Y-Xb%*%bhatb)
s2b <- as.numeric(rssb)/(nobs-k)
Cb <- solve(t(Xb)%*%Xb)

# Find the diagonals of the C matrix
cb_ii <- diag(Cb)
# Calculate the standard error
std.errorb <- sqrt(s2b*cb_ii)
### Using lm
mod1 <- lm(sales~TV+radio+newspaper, data=adverts)
mod2 <- lm(sales~., data=adverts)
mod3 <- lm(sales ~ TV, data = adverts)

```


## Introduction

Linear regression techniques are tools that can be used for prediction and inference purposes. Such tools help us to answer questions such as: how does my predictor variable, $X_1$, affect my response variable $Y$? how accurately can I estimate this influence? etc... We also saw that simple linear models can be extended to include many predictor variables. 

The general model formulation is given as:
$$\boldsymbol{Y} = \boldsymbol{X\beta} + \boldsymbol{e}$$
where $\boldsymbol{Y}$ is a continuous predictor variable, $\boldsymbol{X}$ is a matrix of $p$ predictor variables and a column of 1's, and $\boldsymbol{e}\sim N(\boldsymbol 0, \sigma^2 \boldsymbol I )$. We also assume a linear relationship between the response variable and the predictor variable. 

In this tutorial, we will implement the linear regression in **R** using the Advertising data set. 


Before proceeding with the modelling part, it is important to understand what your data looks like. Is your response variable continuous? How about your predictor variables?

## Exploratory data analysis


```{r adverts, include=FALSE,eval=TRUE}
adverts <- read.csv("Advertising.csv", header=TRUE)[,-1]
attach(adverts)
```

### View the data

The `Advertising` data set consists of sales data recorded across 200 different markets. The following variables have been recorded: 

- Sales (No. of units sold)
- TV, Newspaper, Radio (Advertising budget in $1000)

The data has already been imported and is stored in the object named `adverts`. Use the `head()` function to view the first few rows of the data frame. 


```{r head, exercise=TRUE}

```

```{r head-solution}
head(adverts)
```

Use the `nrow()` function to find the number of observations, $n$. Then, assign the number of observations to an object named `nobs`.

```{r nobs, exercise=TRUE}

```

```{r nobs-solution}
nrow(adverts)
nobs <- nrow(adverts)
nobs
```



### Scale of measurements

Answer the following questions:


```{r quiz1}
quiz(
  question("The response variable, sales, is measured on which scale?",
           answer("Categorical with 1 level"),
           answer("Categorical with 2 levels"),
           answer("Continuous", correct = TRUE)
  ),
  question(sprintf("How many predictor variables are there?"),
           answer("1"),
           answer("2"),
           answer("3", correct = TRUE)
  ),
  question("Which of the predictor variables are continous?",
           answer("TV only"),
           answer("TV and newspaper"),
           answer("Newspaper and radio"),
           answer("All", correct = TRUE)
  )
    
)

```

### Histogram 

Plot a histogram of the predictor variable `sales`. Superimpose the histogram with a normal density curve by evaluating the PDF of a normal distribution with the mean and standard deviation of `sales`. Fill in the `...`. 

```{r hist, exercise=TRUE}
hist(...,freq=FALSE, main = "Density of sales")
# Generate a sequence of 100 values from the minimimum to the maximum of sales
xs <- seq(from=... , to=... , length.out=...)
# Find the mean and standard deviation of sales
msales <- mean(...)
sdsales <- sd(...)
# Density values
ys <- dnorm(x=xs, mean=...,sd=...)
# Plot the density line
lines(ys~xs,lwd=2,col='red')
```

```{r hist-hint}
?hist
?seq
?dnorm
```


```{r hist-solution}
hist(sales,freq=FALSE)
# Generate a sequence of values from the minimimum to the maximum of sales of length 100
xs <- seq(from=min(sales),to=max(sales),length.out=100)
# Find the mean and standard deviation of sales
msales <- mean(sales)
sdsales <- sd(sales)
# Density values
ys <- dnorm(x=xs, mean=msales ,sd=sdsales)
# Plot the density line
lines(ys~xs,lwd=2,col='red')

```

### Scatter plots

Illustrate the relationship between `sales` and `TV` using a scatter plot.

```{r scatter, exercise=TRUE}
plot(x= ... , y= ..., xlab = ...., ylab = .... )

```



```{r scatter-solution}
plot(x= adverts$TV , y= adverts$sales, xlab = "TV", ylab = "sales" )

```

You can change the colour of the points by specifying `col="red"` in the function for example.


```{r scatter1, exercise=TRUE}
plot(x= ... , y= ... , col= "..." )

```


It is often more useful to use pairwise plots. This can be done by using the `pairs` function in R. 


```{r pairs, exercise = TRUE}
pairs(...)
```

```{r pairs-solution}
pairs(adverts)
```


### Correlation

You can find the correlation between the **predictor variables** using the `cor()` function in R.

```{r cor, exercise=TRUE}
# Predictor variables (first to third columns): adverts[,1:3]
cor(...)
```

```{r cor-solution}
cor(adverts[,1:3 ])
```


You can also visualise the correlation matrix using a correlogram. This can be done using the function `corrplot()` from the **corrplot** package.


```{r correlogram, exercise = TRUE}
corr <- cor(adverts[,1:3 ])
corrplot(corr, method = "circle", type = "lower")
# Plots the lower triangular matrix. type  = "upper" plots the upper triangular matrix.
corrplot(corr, method = "number", type = "lower")
```

## Simple linear regression

Let's look at a simple regression model with one predictor variable, TV. The model is given by:

$$ \boldsymbol{Y} = \beta_0 \boldsymbol{1} + \beta_1 \boldsymbol X_1 + \boldsymbol e$$

where $\boldsymbol Y$ is your sales data and $\boldsymbol{X_1}$ is the advertising budget spent on TV. 

### Construct X

To construct the $\boldsymbol X = [\boldsymbol{1 \; X_1}]$ matrix, we column bind a vector of ones' with the TV vector. You can do this by using the `cbind()` function in R.

```{r X, exercise=TRUE}
X <- cbind(...,..)
head(X)
```

```{r X-hint}
?cbind
```

```{r X-solution}
X <- cbind(1, adverts$TV)
head(X)
```


### Estimate the regression coefficients

We found that by using the maximum likelihood estimation, an estimator for $\boldsymbol \beta$ is given by 

$$\widehat{\boldsymbol \beta}=  (\boldsymbol{X^TX})^{-1}\boldsymbol{X^TY}$$

Assign `sales` to object `Y`. Find the regression coefficients using the formula above in R. 
Recall: You can multiply two matrices using the operator `%*%`. You can inverse a matrix using `solve()`, and you can transpose a matrix using `t()`.

```{r estsimple, exercise=TRUE}
Y <- ...
bhat <- ...
bhat
```


```{r estsimple-solution}
Y <- adverts$sales
bhat <- solve(t(X)%*%X)%*%t(X)%*%Y
bhat
```


### Standard error of the regression coeffcients

We also found that the regression coefficents are multivariate normally distributed:


$$\widehat{\boldsymbol{\beta}}\sim \mathcal{N}_k(\boldsymbol{\beta}, \sigma^2 \left(\boldsymbol{X^TX}\right)^{-1})$$

where $k=p+1$

The standard error of the regression coefficients is given by the square root of the diagonal elements of $\sigma^2 (\boldsymbol{X^TX})^{-1}$. 

Let $\boldsymbol{C} = (\boldsymbol{X^TX})^{-1}$. For a bivariate case, 
$\boldsymbol{C} = \left(\begin{matrix} c_{11} & c_{12} \\ c_{21} & c_{22} \end{matrix} \right)$, therefore $SE(\hat{\beta_0}) = \sqrt{\sigma^2 c_{11}}$ and  $SE(\hat{\beta_1}) = \sqrt{\sigma^2 c_{22}}$. 

Find the matrix $\boldsymbol{C}$:

```{r cmat, exercise=TRUE}
C <- ...
C
```

```{r cmat-solution}
C <- solve(t(X)%*%X)
C
```

#### Estimate the residual variance

We first need to estimate $\sigma^2$. Recall that an unbiased estimate for $\sigma^2$ is given by:

$$ s^{2} = \frac{1}{n-k}(\mathbf{Y-X\hat{\boldsymbol \beta})}^T\mathbf{(Y}-\mathbf{
X\hat{\boldsymbol \beta})} $$

where $k=p+1$

Now, use R to find $s^2$.

```{r s2, exercise=TRUE}
# Find k
k <- ...
# Find s2
s2 <- ... 
s2
```

```{r s2-solution}
# Find k
k <- 1+1
# Find s2

# First calculate the residuals sum of squares
rss <- t(Y-X%*%bhat)%*%(Y-X%*%bhat)

# Then, calculate s2. 
s2 <- as.numeric(rss)/(nobs-k)
```


Now, find the standard error of the regression coefficients.

```{r stderr, exercise=TRUE}
# Find the diagonals of the C matrix
c_ii <- diag(...)
# Calculate the standard error
std.error <- sqrt(...)
std.error
```

```{r stderr-solution}
# Find the diagonals of the C matrix
c_ii <- diag(C)
# Calculate the standard error
std.error <- sqrt(s2*c_ii)
std.error
```

Find the standard error of $\hat{\beta_1}$.

```{r stderr1, exercise=TRUE}
b1.err <- std.error[...]
```

```{r stderr1-solution}
b1.err <- std.error[2]
b1.err
```


In the next topic, we will look at the multiple regression model.

## Multiple linear regression

We are often more interested in the relationship between a response variable and multiple predictor variables. The theory remain the same as in the simple linear model, the only difference being the $\boldsymbol X$ matrix (also called the design matrix) which consists of multiple predictor variables.

Let's implement a multiple regression model to include the following variables: TV, newspaper, radio.

First, construct the design matrix, $\boldsymbol{X}$ and call it `Xb`. 

```{r des, exercise=TRUE}
Xb <- as.matrix(cbind(...))
head(Xb)
```

```{r des-solution}
Xb <- as.matrix(cbind(1, adverts$TV, adverts$radio, adverts$newspaper))
# or 
Xb <- as.matrix(cbind(1,adverts[,1:3]))
```


### Estimating the regression parameters

Similarly,

$$\widehat{\boldsymbol \beta}=  (\boldsymbol{X^TX})^{-1}\boldsymbol{X^TY}$$
Find $\widehat{\boldsymbol \beta}$ and name it `bhatb`. 

```{r bhat2, exercise=TRUE}
bhatb <- ... 
bhatb
```

```{r bhat2-solution}
bhatb <-solve(t(Xb)%*%Xb)%*%t(Xb)%*%Y
```
`


### Estimate the residual variance

Recall that an unbiased estimate for $\sigma^2$ is given by:

$$ s^{2} = \frac{1}{n-k}(\mathbf{Y-X\hat{\boldsymbol \beta})}^T\mathbf{(Y}-\mathbf{
X\hat{\boldsymbol \beta})} $$



Now, use R to find $s^2$ and call it `s2b`. 

```{r s2b, exercise=TRUE}
# Find k
k <- ...
# Find s2
s2b <- ... 
s2b

```


```{r s2b-solution}
# Find k
k <- 1+3
# Find s2
rssb <- t(Y-Xb%*%bhatb)%*%(Y-Xb%*%bhatb)
s2b <- as.numeric(rssb)/(nobs-k)
```



### Standard error of the regression coefficients

Using the same principles as the simple linear model, find the standard error of the regression coefficients.

```{r stderrb, exercise=TRUE}
# Find the C matrix
Cb <- ...
# Find the diagonals of the C matrix
cb_ii <- diag(...)
# Calculate the standard error
std.errorb <- sqrt(...)
std.errorb
```

```{r stderrb-solution}
# Find the diagonals of the C matrix
cb_ii <- diag(Cb)
# Calculate the standard error
std.errorb <- sqrt(s2b*cb_ii)
std.errorb
```




In the above exercises, we did not use any built-in R functions to build our regression models. It can be much quicker and easier to use R functions to perform such tasks. Nevertheless, it is important to be aware of what those functions do i.e., how the results were obtained and what do they mean. 

The base R `stats` package contains functions for statistical calculations and random number generation. It also contains built-in functions to build linear models. In the next topic, we will implement a multiple regression model using the `lm()` function.

## Linear models using lm()

We can use the `lm()` function to build a linear model with one or more predictor variables. 

```{r lm,exercise=TRUE}
help(lm)
```


The function by default includes the intercept term. We can specify the model by setting `formula = Y ~ X1 + X2 + ... + Xp` where $X_i$'s are the predictor variables. `data` must also be set to the data frame you are using.

Let's implement this using the advertising data set.

Fit a linear model with the predictor variables TV, newspaper and radio using the `lm()` function. Name the object `mod1`.

```{r lm1, exercise=TRUE}

mod1 <- lm(formula = ... ~ ... + ... + ..., data = ... )

```

```{r lm1-solution}
mod1 <- lm(sales~TV+radio+newspaper, data=adverts)
mod1 
```


A trick: By setting `formula=sales ~.`, the function sets the all of the remaining variables in the data frame as the predictor variables. 


```{r lm2, exercise =TRUE}
mod2 <- lm(sales~.,data=adverts)
mod2
```

We can obtain the regression coefficients (`estimate`) and standard error using the `summary()` function. 

```{r summ, exercise=TRUE}
summary(mod1)
```




Using the lm() function, build a regression model using TV as the only predictor variable. 

```{r TVonly, eval = TRUE, exercise = TRUE}
mod3 <- lm(sales ~ TV, data = adverts)
```

Extract and plot the fitted values $\hat{\boldsymbol Y} =  \boldsymbol X \hat{\boldsymbol \beta}$:

```{r fit, exercise = TRUE}
yhat =  fitted(mod3)
plot(x=adverts$TV,y=adverts$sales,col="blue", pch = 16, ylab = "sales", xlab = "TV") # Observed
lines(yhat ~ adverts$TV,col='red',lwd=2) # Fitted values

```


Extract the residuals $\hat{\boldsymbol e} = \boldsymbol Y-\hat{\boldsymbol Y}$.

```{r res, exercise = TRUE}
ehat = mod3$residuals
head(ehat)
sum(ehat^2) # Residuals sums of squares
```

